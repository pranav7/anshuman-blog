{
  
    
        "post0": {
            "title": "Building a basic Neural Network from scratch - Kudzu",
            "content": "In this project, the idea is to write a classifier to differentiate between handwritten digits 3 and 8, from the MNIST database. . I will using a custom built neural network library called Kudzu.This has been developed as a part of KTF/Foundations course of Univ.ai (www.univ.ai). The source code for Kudzu Library and and this notebook can be found on my git-hub profile : www.https://github.com/anshuman6 under the folder project-solution. Any feedback and comments are welcome. Please feel free to email me at: anshuman6@gmail.com . 1. I will be using the MNIST database. . 2. I will be comparing the results of the 4 layer NN to a standard logistic regression . Importing all the necessary libraries . %load_ext autoreload %autoreload 2 . %matplotlib inline import numpy as np import matplotlib.pyplot as plt import pandas as pd . The following command helps us download MNIST from notebook itself. You can skip this if you already have MNIST. You can also download it via your terminal. . !pip install mnist # Please note, it is commented out for now, you can remove comment if you want to install it . Collecting mnist Downloading mnist-0.2.2-py2.py3-none-any.whl (3.5 kB) Requirement already satisfied: numpy in /opt/hostedtoolcache/Python/3.6.11/x64/lib/python3.6/site-packages (from mnist) (1.19.1) Installing collected packages: mnist Successfully installed mnist-0.2.2 WARNING: You are using pip version 20.2; however, version 20.2.1 is available. You should consider upgrading via the &#39;/opt/hostedtoolcache/Python/3.6.11/x64/bin/python -m pip install --upgrade pip&#39; command. . Preparing the Data . import mnist . train_images = mnist.train_images() train_labels = mnist.train_labels() . train_images.shape, train_labels.shape . ((60000, 28, 28), (60000,)) . test_images = mnist.test_images() test_labels = mnist.test_labels() . test_images.shape, test_labels.shape . ((10000, 28, 28), (10000,)) . image_index = 7776 # You may select anything up to 60,000 print(train_labels[image_index]) plt.imshow(train_images[image_index], cmap=&#39;Greys&#39;) . 2 . &lt;matplotlib.image.AxesImage at 0x7f2befc28a20&gt; . Filter data to get 3 and 8 out . train_filter = np.where((train_labels == 3 ) | (train_labels == 8)) test_filter = np.where((test_labels == 3) | (test_labels == 8)) X_train, y_train = train_images[train_filter], train_labels[train_filter] X_test, y_test = test_images[test_filter], test_labels[test_filter] . We normalize the pizel values in the 0 to 1 range . X_train = X_train/255. X_test = X_test/255. . And setup the labels as 1 (when the digit is 3) and 0 (when the digit is 8) . y_train = 1*(y_train==3) y_test = 1*(y_test==3) . X_train.shape, X_test.shape . ((11982, 28, 28), (1984, 28, 28)) . We reshape the data to flatten the image pixels into a set of features or co-variates: . X_train = X_train.reshape(X_train.shape[0], -1) X_test = X_test.reshape(X_test.shape[0], -1) X_train.shape, X_test.shape . ((11982, 784), (1984, 784)) . Importing Kudzu library and its functionality . from kudzu.data import Data, Dataloader, Sampler from kudzu.callbacks import AccCallback from kudzu.loss import MSE from kudzu.layer import Affine, Sigmoid from kudzu.model import Model from kudzu.optim import GD from kudzu.train import Learner from kudzu.callbacks import ClfCallback from kudzu.layer import Sigmoid from kudzu.layer import Relu . We are creating a class, just so that we can use it to store our parameters for us . class Config: pass config = Config() config.lr = 0.001 config.num_epochs = 250 config.bs = 50 . Initializing Data . data = Data(X_train, y_train.reshape(-1,1)) loss = MSE() opt = GD(config.lr) sampler = Sampler(data, config.bs, shuffle=True) dl = Dataloader(data, sampler) . Constructing 2 different layers, one for NN and one only for logistic regression. . Creating containers for data to be passed, to calculate accuracies . training_xdata = X_train testing_xdata = X_test training_ydata = y_train.reshape(-1,1) testing_ydata = y_test.reshape(-1,1) . Initializing models and Running training loop: . layers = [Affine(&quot;first&quot;, 784, 100), Relu(&quot;first&quot;), Affine(&quot;second&quot;, 100, 100), Relu(&quot;second&quot;), Affine(&quot;third&quot;, 100, 2), Affine(&quot;final&quot;, 2, 1), Sigmoid(&quot;final&quot;)] model_neural = Model(layers) model_logistic = Model([Affine(&quot;logits&quot;, 784, 1), Sigmoid(&quot;sigmoid&quot;)]) . learner1 = Learner(loss, model_neural, opt, config.num_epochs) acc1 = ClfCallback(learner1, config.bs, training_xdata , testing_xdata, training_ydata, testing_ydata) learner1.set_callbacks([acc1]) . learner1.train_loop(dl) . Epoch 0 Loss 0.2768642046331115 train accuracy is: 0.5766149223835754, test accuracy is 0.577116935483871 Epoch 10 Loss 0.06857490369543898 train accuracy is: 0.92997830078451, test accuracy is 0.938508064516129 Epoch 20 Loss 0.04639458457167084 train accuracy is: 0.9498414288098815, test accuracy is 0.9596774193548387 Epoch 30 Loss 0.038418798615304936 train accuracy is: 0.956935403104657, test accuracy is 0.9632056451612904 Epoch 40 Loss 0.03414292951213293 train accuracy is: 0.9613587047237523, test accuracy is 0.9652217741935484 Epoch 50 Loss 0.03136675700809557 train accuracy is: 0.9644466700050075, test accuracy is 0.9667338709677419 Epoch 60 Loss 0.029377155226115247 train accuracy is: 0.9663662159906526, test accuracy is 0.9682459677419355 Epoch 70 Loss 0.027848880956106943 train accuracy is: 0.9683692204974128, test accuracy is 0.967741935483871 Epoch 80 Loss 0.026649574486672347 train accuracy is: 0.9689534301452178, test accuracy is 0.9692540322580645 Epoch 90 Loss 0.02563137531963582 train accuracy is: 0.970372225004173, test accuracy is 0.969758064516129 Epoch 100 Loss 0.02475588728605902 train accuracy is: 0.971373727257553, test accuracy is 0.9702620967741935 Epoch 110 Loss 0.02397895778552856 train accuracy is: 0.972291770989818, test accuracy is 0.9702620967741935 Epoch 120 Loss 0.023284355428885867 train accuracy is: 0.9732932732431981, test accuracy is 0.9702620967741935 Epoch 130 Loss 0.022658349631263588 train accuracy is: 0.9738774828910032, test accuracy is 0.9702620967741935 Epoch 140 Loss 0.022088933304300187 train accuracy is: 0.9744616925388082, test accuracy is 0.9712701612903226 Epoch 150 Loss 0.021557078447335756 train accuracy is: 0.9752128192288433, test accuracy is 0.9722782258064516 Epoch 160 Loss 0.021072415401257403 train accuracy is: 0.9759639459188784, test accuracy is 0.9727822580645161 Epoch 170 Loss 0.020614959477414648 train accuracy is: 0.9768819896511434, test accuracy is 0.9727822580645161 Epoch 180 Loss 0.020179391536275532 train accuracy is: 0.9775496578200634, test accuracy is 0.9727822580645161 Epoch 190 Loss 0.019763242344585756 train accuracy is: 0.9781338674678685, test accuracy is 0.9722782258064516 Epoch 200 Loss 0.019368712959178355 train accuracy is: 0.9783007845100985, test accuracy is 0.9727822580645161 Epoch 210 Loss 0.01898160014739736 train accuracy is: 0.9787180771156735, test accuracy is 0.9732862903225806 Epoch 220 Loss 0.018612771190808286 train accuracy is: 0.9792188282423635, test accuracy is 0.9732862903225806 Epoch 230 Loss 0.018244249268139032 train accuracy is: 0.9796361208479386, test accuracy is 0.9732862903225806 Epoch 240 Loss 0.017881680765552885 train accuracy is: 0.9803872475379736, test accuracy is 0.9737903225806451 . 0.03152114743366363 . Now running only the logistic regression based classification to compare results with NN . learner2 = Learner(loss, model_logistic, opt, config.num_epochs) acc2 = ClfCallback(learner2, config.bs, training_xdata , testing_xdata, training_ydata, testing_ydata) learner2.set_callbacks([acc2]) . learner2.train_loop(dl) . Epoch 0 Loss 0.22483520757673633 train accuracy is: 0.7330996494742114, test accuracy is 0.7555443548387096 Epoch 10 Loss 0.10460906110405843 train accuracy is: 0.900433984309798, test accuracy is 0.9158266129032258 Epoch 20 Loss 0.08094861623115167 train accuracy is: 0.9222166583208146, test accuracy is 0.9359879032258065 Epoch 30 Loss 0.0696721491583139 train accuracy is: 0.9318143882490403, test accuracy is 0.9440524193548387 Epoch 40 Loss 0.06282404623942289 train accuracy is: 0.9380737773326656, test accuracy is 0.9475806451612904 Epoch 50 Loss 0.05813896361093811 train accuracy is: 0.942663995993991, test accuracy is 0.952116935483871 Epoch 60 Loss 0.05469067354034763 train accuracy is: 0.9453346686696712, test accuracy is 0.9561491935483871 Epoch 70 Loss 0.052023472574362316 train accuracy is: 0.9472542146553163, test accuracy is 0.9581653225806451 Epoch 80 Loss 0.04988625240140377 train accuracy is: 0.9498414288098815, test accuracy is 0.9586693548387096 Epoch 90 Loss 0.048126187687430086 train accuracy is: 0.9512602236688366, test accuracy is 0.9601814516129032 Epoch 100 Loss 0.04664348290990215 train accuracy is: 0.9520113503588716, test accuracy is 0.9601814516129032 Epoch 110 Loss 0.04537250070461334 train accuracy is: 0.9532632281755967, test accuracy is 0.9611895161290323 Epoch 120 Loss 0.04427034482279185 train accuracy is: 0.9543481889500918, test accuracy is 0.9616935483870968 Epoch 130 Loss 0.04329961185874676 train accuracy is: 0.9550158571190118, test accuracy is 0.9611895161290323 Epoch 140 Loss 0.042438247282348726 train accuracy is: 0.9559339008512769, test accuracy is 0.9611895161290323 Epoch 150 Loss 0.04166523316663205 train accuracy is: 0.9567684860624269, test accuracy is 0.9611895161290323 Epoch 160 Loss 0.04096841553995623 train accuracy is: 0.957185778668002, test accuracy is 0.9616935483870968 Epoch 170 Loss 0.040334226653092733 train accuracy is: 0.9577699883158071, test accuracy is 0.9627016129032258 Epoch 180 Loss 0.039754502998538305 train accuracy is: 0.9584376564847271, test accuracy is 0.9632056451612904 Epoch 190 Loss 0.03922190152592479 train accuracy is: 0.9588549490903021, test accuracy is 0.9632056451612904 Epoch 200 Loss 0.03872980040517251 train accuracy is: 0.9591053246536472, test accuracy is 0.9632056451612904 Epoch 210 Loss 0.038274209818047994 train accuracy is: 0.9594391587381071, test accuracy is 0.9632056451612904 Epoch 220 Loss 0.03785020807205841 train accuracy is: 0.9595226172592222, test accuracy is 0.9642137096774194 Epoch 230 Loss 0.037453997156249676 train accuracy is: 0.9598564513436821, test accuracy is 0.9637096774193549 Epoch 240 Loss 0.037083128289679794 train accuracy is: 0.9600233683859122, test accuracy is 0.9637096774193549 . 0.017281676616608184 . Comparing results of NN and LR . plt.figure(figsize=(8,5)) plt.plot(acc1.val_accuracies, &#39;g-&#39;, label = &quot;Val Accuracies - NN&quot;) plt.plot(acc1.accuracies, &#39;r-&#39;, label = &quot;Accuracies - NN&quot;) plt.plot(acc2.val_accuracies, &#39;b-&#39;, label = &quot;Val Accuracies - Logistic Reg&quot;) plt.plot(acc2.accuracies, &#39;k-&#39;, label = &quot;Accuracies - Logistic Reg&quot;) plt.ylim(0.8,1) ## for a more spread out view plt.legend() . &lt;matplotlib.legend.Legend at 0x7f2bef8150f0&gt; . Clearly NN has a better accuracy over LR; NN is overfitting - Validation accuracy (green) has dropped below training accuracy (orange), also they are diverging. This problem is not seen in LR . Now we will be moving through the network, till the output of the second last affine where we get a 2 dimensional output. We will be plotting this 2d output and probability contours . model_new = Model(layers[:-2]) . plot_testing = model_new(testing_xdata) . Plotting the scatter plot of points and color coding by class . plt.figure(figsize=(8,7)) plt.scatter(plot_testing[:,0], plot_testing[:,1], alpha = 0.1, c = y_test.ravel()); . Plotting probability contours . model_prob = Model(layers[-2:]) ## picking only last two layers to get probability. That is affine followed by sigmoid . xgrid = np.linspace(-4, 1, 100) ## Adjust these values based on above chart, roughly -4 to 1 ygrid = np.linspace(-7.5, 7.5, 100) ## Adjust these values based on above chart, roughly -7.5, 7.5 xg, yg = np.meshgrid(xgrid, ygrid) # xg and yg are now both 100X100, lets convert them to single arrays xg_interim = np.ravel(xg) yg_interim = np.ravel(yg) ## xg_interim, yg_interim are now arrays of len 10000, now we will stack them and then transpose to get desired shape of n rows, 2 columns X_interim = np.vstack((xg_interim, yg_interim)) ## Please note vstack takes in a tuple X = X_interim.T ## We want a shape of n rows and 2 columns to be able to feed this to last affine ## This last affine takes only two columns, hence the above transformation probability_contour = model_prob(X).reshape(100,100) ## to make it consistent with xg, yg . plt.figure(figsize=(8,7)) plt.scatter(plot_testing[:,0], plot_testing[:,1], alpha = 0.1, c = y_test.ravel()) contours = plt.contour(xg,yg,probability_contour) plt.clabel(contours, inline = True ); .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/07/NN-new.html",
            "relUrl": "/2020/08/07/NN-new.html",
            "date": " • Aug 7, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Dynamic Covid-19 Tracker",
            "content": "Hellow!, This is a dynamic version of the dashboard, it updates once daily!!! . #collapse from datetime import datetime import pandas as pd import numpy as np import requests import json import matplotlib.pyplot as plt import matplotlib.dates as mdates import matplotlib as mpl from IPython.core.display import display,HTML %matplotlib inline dynamic_df = pd.read_csv(&quot;https://api.covid19india.org/csv/latest/state_wise_daily.csv&quot;) dynamic_df.head() ddf = dynamic_df[(dynamic_df.Status == &quot;Confirmed&quot;)] ddf ddf1 = ddf.drop(columns = [&quot;Status&quot;]) ddf2 = dynamic_df[(dynamic_df.Status == &quot;Deceased&quot;)] ddf2 = ddf2.drop(columns = [&quot;Status&quot;]) ddf1[&quot;Date&quot;] = ddf1[&quot;Date&quot;].astype(&#39;datetime64[ns]&#39;) update = dynamic_df.iloc[-1,0] cases = ddf1.TT.sum() new = ddf1.iloc[-1,1] deaths = ddf2.TT.sum() dnew = ddf2.iloc[-1,1] overview = &#39;&#39;&#39; &lt;!-- ####### HTML!! #########--&gt; &lt;h1 style=&quot;color: #5e9ca0; text-align: center;&quot;&gt;India&lt;/h1&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Last update: &lt;strong&gt;{update}&lt;/strong&gt;&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed cases:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{cases} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{new}&lt;/span&gt;)&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed deaths:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{deaths} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{dnew}&lt;/span&gt;)&lt;/p&gt; &#39;&#39;&#39; html = HTML(overview.format(update=update, cases=cases,new=new,deaths=deaths,dnew=dnew)) display(html) . . India . Last update: 08-Aug-20 . Confirmed cases: . 2151928 (+65156) . Confirmed deaths: . 43464 (+875) . #collapse now = datetime.now().time() # time object print(&quot;This dashboard is last updated at =&quot;, now) . . This dashboard is last updated at = 10:10:46.393066 . #collapse n = 10 st = [&quot;TT&quot;, &quot;MH&quot;, &quot;TN&quot;, &quot;DL&quot;, &quot;KA&quot;, &quot;UP&quot;, &quot;BR&quot;, &quot;WB&quot;, &quot;TG&quot;, &quot;CH&quot;] st_name = [&quot;Daily Count for India&quot;, &quot;Maharashta&quot;, &quot;Tamil Nadu&quot;, &quot;Delhi&quot;, &quot;Karnataka&quot;, &quot;Uttar Pradesh&quot;, &quot;Bihar&quot;, &quot;West Bengal&quot;, &quot;Telangana&quot;, &quot;Chandigarh (My hometown)&quot;] ax = [] fig = plt.figure(figsize = (16,30)) gs = fig.add_gridspec(n, 3) for i in range(n): ax1 = fig.add_subplot(gs[i, :]) ax1.bar(ddf1.Date,ddf1[st[i]],alpha=0.3,color=&#39;#007acc&#39;) ax1.plot(ddf1.Date,ddf1[st[i]] , marker=&quot;o&quot;, color=&#39;#007acc&#39;) ax1.xaxis.set_major_locator(mdates.WeekdayLocator()) ax1.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax1.text(0.02, 0.5,st_name[i], transform = ax1.transAxes, fontsize=25) ax1.spines[&#39;right&#39;].set_visible(False) ax1.spines[&#39;top&#39;].set_visible(False) . .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/06/dynamic.html",
            "relUrl": "/2020/08/06/dynamic.html",
            "date": " • Aug 6, 2020"
        }
        
    
  
    
        ,"post2": {
            "title": "The devil is in the details (Bayes Theorem and Covid-19)",
            "content": "Serological surveys refer to conducting antibody tests on populations to ascertain how many people already have antibodies against coronavirus (Covid-19). This means at some point they have already contracted coronavirus and may/may-not have shown symptoms. In either situation, presence of antibodies can suggest some form of immunity against future infections from coronavirus (to what extent though, is still to be scientifically proven). . Serological surveys can give useful information to scientists about the spead patterns of the disease. One school of thought also suggests that people with antibodies &quot;might&quot; be at lower risk to return back to workforce. . Serological tests are different from RT-PCR tests (reverse transcriptase polymerase chain reaction), you might them as the &#39;swab&#39; tests. Swab tests are very different from antibody tests. Swab test tells you if the host has the virus at present, antibody test tells you the hosts response to the virus - by checking if host as antibodies against the test, giving indication that the host might have recently or in the near past, might have contracted coronavirus. . When it comes to any form of testing, there are three important parameters to understand: Sensitivity and Specificity and a &#39;Third One&#39;, which we will come to later. . Sensitivity basically tells you, what are the chances that if you test positive, you actually have the disease/antibodies (depending on what you are testing for). (Think of it as True positive Rate). You will usually see tests claiming different sensitivity values - 98%, 99% etc. What this means is that if you get tested positive, there is a 98% chance that you have the disease/antibodies. What this also means is that 2% of the times, you will get a &#39;false positive&#39; - that is, you get a positive, although you dont have the diesease/antibodies. . This can be a little dangerous, if you fall in that 2% zone of an antibody test, you might think that you already have the antibodies and are at a lower risk, however, this might not be true and you might be putting yourself at risk if you go out and expose yourself more. . Specificity on the other hand tells you, what are the chances that if you test negative, you actually do not have the disease/antibodies (depending on what you are testing for). (Think of it as True negative Rate). If specificity is 98%, 2% of people will test negative, but will have the disease/antibodies. So if you get a test done, and you tested negative, there is still chance that you might have the disease (or antibodies)! So 2% of the times you have the possibility of getting a &#39;false negative&#39;. . Now coming to the third parameter. Theoretically, If you are able to somehow get your hands on a test which claims high sensitivity and specificty, can you rule out the possibility of false positives and false negatives? Unfortunately not. . Unless you completely understand, how the manufacturer of the test calculated these claimed specificty and sensitivity values, you cannot be sure. . Results of tests, in the real world, vary from person to person. Someone might have high number of antibodies and the test might catch it easily, someone might not and the test might fail. These claimed values need to be tested from large samples of populations (sampled correctly keeping various randomizing factors in mind), before these claimed values can be trusted. However, unfortunately, not a lot of test manufacturers disclose actual details of how these claimed values of specificity and sensitivity are calculated. . Not to go very deep, but there is one more factor to take into consideration - it is the disease prevalence rate. How widespread is the disease in the general population. Do a lot of people have it? If the prevalence is low, the chances of getting false positives are higher. You might test positive, but you might not have the disease. If prevalence is high, you might test negative, but still have the disease! . So in conclusion, the devil is in the details. Diagnostic science is not 100 percent foolproof, but with the combinations of different modalities, you can be more certain. . . Below is a python snippet and a visualization showcasing the point about prevalence and its impact on false positives. . So if you are in a place where a disease is not prevalent, and you test positive for it. There are chances that it might be a false positive and you might need to test again using other modalities to confirm and re-confirm. This is if you are towards the left side of the chart below. . import numpy as np import matplotlib.pyplot as plt s1 = .90 # P(T+|D+) s2 = 0.70 # P(T+|D+) spec = .98 # P(T-|D-) dplus = np.arange(0,1,0.01) pdneg = 1 - dplus # Using magical bayes theorem equations - # at 90% dplustplus1 = (s1 * dplus) / ((s1 * dplus) + ((1-spec)*pdneg)) # at 70% dplustplus2 = (s2 * dplus) / ((s2* dplus) + ((1-spec)*pdneg)) plt.figure(figsize = (10,8)) plt.plot(dplus,dplustplus1, &#39;g&#39;, dplus,dplustplus2,&#39;b&#39;) plt.xlabel(&quot;Prevalence&quot;,fontsize = 17) plt.ylabel(&quot;If tested positive, chance that you actually have the disease&quot;, fontsize = 15) plt.title(&quot;Green - 90% sensitivity, Blue - 70% Sensitivity&quot;, fontsize = 17) plt.show() .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/01/surveys.html",
            "relUrl": "/2020/08/01/surveys.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post3": {
            "title": "Quick PDF, CDF Implementation in code",
            "content": "Just some quick code to understand PDF and CDF from a code/viz perspective, for when math equations can look scary . # Import all the nice packages import numpy as np import math import matplotlib.pyplot as plt import scipy.integrate as integrate import scipy.stats . # Decide on a mean and sigma for the gaussian dist. mean = 0 sigma = 2.5 # Generate random data to plot the gaussian x = np.arange(-10,10,0.1) . y_values = scipy.stats.norm(mean, sigma) y_values_pdf = y_values.pdf(x) y_values_cdf = y_values.cdf(x) # Nice function to calculate integral area = integrate.quad(lambda x: y_values.pdf(x), -10, 0) area[0] area_str = str(round(area[0], 2)) . # Matplotlib magic plt.figure(figsize = (15,6)) plt.subplot(1,2,1) plt.plot(x, y_values_pdf) plt.text(-10, 0.1, &quot;Area in highlighted region = probability of getting 0 or less = {}&quot;.format(area_str)) plt.fill_between(x, y_values_pdf, where = (x&lt;0), color = &quot;g&quot;) plt.title(&quot;Probability Density Function&quot;) plt.subplot(1,2,2) plt.plot(x, y_values_cdf) plt.axhline(y=0.5) plt.axvline(x=0) plt.title(&quot;Cumulative Density Function&quot;) plt.show() . Area under PDF is the probability. Any point on the CDF is the the probability of getting that value OR less. . Example - The shaded green region is the probability of getting 0 or less from PDF &gt; 0.5 in this case (half of fig is shaded) . If you draw a vertical line at 0, it intersects the CDF at 0.5 -&gt; shows probability of getting 0 OR less is 0.5 .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/01/gaussian.html",
            "relUrl": "/2020/08/01/gaussian.html",
            "date": " • Aug 1, 2020"
        }
        
    
  
    
        ,"post4": {
            "title": "Covid-19 Tracker",
            "content": "Hello, welcome to my dashboard. I have created it using matplotlib. This is static. You can collapse cells (&quot;Show Code&quot;) if you want to review the code . #collapse import pandas as pd import numpy as np import requests import json import matplotlib.pyplot as plt import matplotlib.dates as mdates import matplotlib as mpl from IPython.core.display import display,HTML %matplotlib inline dft_cases = pd.read_csv(&#39;data/SnapshotCases-28-July.csv&#39;) dft_deaths = pd.read_csv(&#39;data/SnapshotDeaths-28-July.csv&#39;) dft_cases[&quot;dt_today&quot;] = dft_cases[&quot;28-Jul-20&quot;] dft_cases[&quot;dt_yday&quot;] = dft_cases[&quot;27-Jul-20&quot;] dft_deaths[&quot;dt_today&quot;] = dft_deaths[&quot;28-Jul-20&quot;] dft_deaths[&quot;dt_yday&quot;] = dft_deaths[&quot;27-Jul-20&quot;] dfc_cases = dft_cases.groupby(&#39;states&#39;)[&quot;dt_today&quot;].sum() dfc_deaths = dft_deaths.groupby(&#39;states&#39;)[&quot;dt_today&quot;].sum() dfp_cases = dft_cases.groupby(&#39;states&#39;)[&quot;dt_yday&quot;].sum() dfp_deaths = dft_deaths.groupby(&#39;states&#39;)[&quot;dt_yday&quot;].sum() df_dfc_cases = pd.DataFrame(dfc_cases).reset_index().rename(columns={&quot;states&quot;: &quot;states&quot;, &quot;dt_today&quot;: &quot;Cases&quot;}) df_dfc_deaths = pd.DataFrame(dfc_deaths).reset_index().rename(columns={&quot;states&quot;: &quot;states&quot;, &quot;dt_today&quot;: &quot;Deaths&quot;}) df_dfp_cases = pd.DataFrame(dfp_cases).reset_index().rename(columns={&quot;states&quot;: &quot;states&quot;, &quot;dt_yday&quot;: &quot;PCases&quot;}) df_dfp_deaths = pd.DataFrame(dfp_deaths).reset_index().rename(columns={&quot;states&quot;: &quot;states&quot;, &quot;dt_yday&quot;: &quot;PDeaths&quot;}) df_table = pd.merge(df_dfc_cases,df_dfp_cases, how=&#39;outer&#39;) df_table = pd.merge(df_table,df_dfc_deaths, how=&#39;outer&#39;) df_table = pd.merge(df_table,df_dfp_deaths, how=&#39;outer&#39;) for c in &#39;Cases, Deaths&#39;.split(&#39;, &#39;): df_table[f&#39;{c} (+)&#39;] = (df_table[c] - df_table[f&#39;P{c}&#39;]).clip(0) df_table[&#39;Fatality Rate&#39;] = (100* df_table[&#39;Deaths&#39;]/ df_table[&#39;Cases&#39;]).round(2) df_table.sort_values(by = [&#39;Cases&#39;,&#39;Deaths&#39;], ascending = [False, False], inplace = True) df_table.reset_index(drop=True, inplace = True) summary = {&quot;updated&quot;:&quot;28th July, 2020&quot;, &quot;since&quot;:&quot;27th July, 2020&quot;} for col in df_table.columns: if col != &quot;states&quot; and col!= &quot;Fatality Rate&quot;: summary[col]= df_table[col].sum() update = summary[&#39;updated&#39;] cases = summary[&#39;Cases&#39;] new = summary[&#39;Cases (+)&#39;] deaths = summary[&#39;Deaths&#39;] dnew = summary[&#39;Deaths (+)&#39;] overview = &#39;&#39;&#39; &lt;!-- ####### HTML!! #########--&gt; &lt;h1 style=&quot;color: #5e9ca0; text-align: center;&quot;&gt;India&lt;/h1&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Last update: &lt;strong&gt;{update}&lt;/strong&gt;&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed cases:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{cases} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{new}&lt;/span&gt;)&lt;/p&gt; &lt;p style=&quot;text-align: center;&quot;&gt;Confirmed deaths:&lt;/p&gt; &lt;p style=&quot;text-align: center;font-size:24px;&quot;&gt;{deaths} (&lt;span style=&quot;color: #ff0000;&quot;&gt;+{dnew}&lt;/span&gt;)&lt;/p&gt; &#39;&#39;&#39; html = HTML(overview.format(update=update, cases=cases,new=new,deaths=deaths,dnew=dnew)) display(html) . . India . Last update: 28th July, 2020 . Confirmed cases: . 1514800 (+49001) . Confirmed deaths: . 34121 (+770) . #collapse dt_cols = list(dft_cases.columns[1:]) dft_ct_new_cases = dft_cases.groupby(&#39;states&#39;)[dt_cols].sum().diff(axis=1).fillna(0).astype(int) dft_ct_new_cases.sort_values(by = &#39;28-Jul-20&#39;, ascending = False,inplace = True) df = dft_ct_new_cases.copy() df.loc[&#39;Total&#39;] = df.sum() df.drop([&#39;dt_today&#39;, &#39;dt_yday&#39;], axis=1, inplace = True) n = 5 ef = df.loc[&#39;Total&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax = [] fig = plt.figure(figsize = (16,20)) gs = fig.add_gridspec(n+2, 3) # gs = fig.add_gridspec(2, 3) ax1 = fig.add_subplot(gs[0, :]) ef = df.loc[&#39;Total&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax1.bar(ef.date,ef.Total,alpha=0.3,color=&#39;#007acc&#39;) ax1.plot(ef.date,ef.Total , marker=&quot;o&quot;, color=&#39;#007acc&#39;) ax1.xaxis.set_major_locator(mdates.WeekdayLocator()) ax1.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax1.text(0.02, 0.5,&#39;India daily case count&#39;, transform = ax1.transAxes, fontsize=25); ax1.spines[&#39;right&#39;].set_visible(False) ax1.spines[&#39;top&#39;].set_visible(False) ax2 = fig.add_subplot(gs[1,0]) ef = df.loc[&#39;Maharashtra&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax2.bar(ef.date, ef.Maharashtra,color = &#39;#007acc&#39;,alpha=0.5) ax2.xaxis.set_major_locator(mdates.WeekdayLocator()) ax2.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax2.set_xticks(ax2.get_xticks()[::3]) maxyval = ef.Maharashtra.max() ax2.set_ylim([0,maxyval]) ax2.text(0.05, 0.5,&#39;Maharashtra&#39;, transform = ax2.transAxes, fontsize=20); ax2.spines[&#39;right&#39;].set_visible(False) ax2.spines[&#39;top&#39;].set_visible(False) ax3 = fig.add_subplot(gs[1,1]) ef = df.loc[&#39;Tamil Nadu&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax3.bar(ef.date, ef[&#39;Tamil Nadu&#39;],color = &#39;#007acc&#39;,alpha=0.5,) ax3.xaxis.set_major_locator(mdates.WeekdayLocator()) ax3.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax3.set_xticks(ax3.get_xticks()[::3]) ax3.text(0.05, 0.5,&#39;Tamil Nadu&#39;, transform = ax3.transAxes, fontsize=20); ax3.spines[&#39;right&#39;].set_visible(False) ax3.spines[&#39;top&#39;].set_visible(False) ax4 = fig.add_subplot(gs[1,2]) ef = df.loc[&#39;Delhi&#39;].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax4.bar(ef.date, ef.Delhi,color = &#39;#007acc&#39;,alpha=0.5) ax4.set_xticks([]) ax4.xaxis.set_major_locator(mdates.WeekdayLocator()) ax4.xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax4.set_xticks(ax4.get_xticks()[::3]) ax4.spines[&#39;right&#39;].set_visible(False) ax4.spines[&#39;top&#39;].set_visible(False) ax4.text(0.05, 0.5,&#39;Delhi&#39;, transform = ax4.transAxes, fontsize=20) for i in range(n): ax.append(fig.add_subplot(gs[i+2,:])) ef = df.iloc[i+3].rename_axis(&#39;date&#39;).reset_index() ef[&#39;date&#39;] = ef[&#39;date&#39;].astype(&#39;datetime64[ns]&#39;) ax[i].bar(ef.date,ef.iloc[:,-1],color = &#39;#007acc&#39;,alpha=0.3) ax[i].plot(ef.date,ef.iloc[:,-1],marker=&#39;o&#39;,color=&#39;#007acc&#39;) ax[i].text(0.02,0.5,f&#39;{ef.columns.values[-1]}&#39;,transform = ax[i].transAxes, fontsize = 20); ax[i].xaxis.set_major_locator(mdates.WeekdayLocator()) ax[i].xaxis.set_major_formatter(mdates.DateFormatter(&#39;%b %d&#39;)) ax[i].set_ylim([0,7000]) ax[i].spines[&#39;right&#39;].set_visible(False) ax[i].spines[&#39;top&#39;].set_visible(False) plt.tight_layout() . . #collapse print(df_table.to_string(index=False)) . . states Cases PCases Deaths PDeaths Cases (+) Deaths (+) Fatality Rate Maharashtra 391440 383723 14164 13882 7717 282 3.62 Tamil Nadu 227688 220716 3659 3571 6972 88 1.61 Delhi 132275 131219 3881 3853 1056 28 2.93 Andhra Pradesh 110297 102349 1148 1090 7948 58 1.04 Karnataka 107001 101465 2064 1962 5536 102 1.93 Uttar Pradesh 73951 70493 1497 1456 3458 41 2.02 West Bengal 62964 60830 1449 1411 2134 38 2.30 Gujarat 57982 56874 2372 2348 1108 24 4.09 Telangana 57142 55532 480 471 1610 9 0.84 Bihar 43591 41111 269 255 2480 14 0.62 Rajasthan 38636 37564 644 633 1072 11 1.67 Assam 34846 33475 92 90 1371 2 0.26 Haryana 32876 32127 406 397 749 9 1.23 Madhya Pradesh 29217 28589 831 821 628 10 2.84 Orissa 28107 26892 189 181 1215 8 0.67 Kerala 20895 19728 68 64 1167 4 0.33 Jammu and Kashmir 18879 18390 333 321 489 12 1.76 Punjab 14378 13769 336 318 609 18 2.34 Jharkhand 9563 8803 94 90 760 4 0.98 Goa 5287 5119 36 36 168 0 0.68 Tripura 4287 4066 21 17 221 4 0.49 Pondicherry 3013 2874 47 43 139 4 1.56 Himachal Pradesh 2330 2270 13 13 60 0 0.56 Manipur 2317 2286 0 0 31 0 0.00 Nagaland 1460 1385 4 5 75 0 0.27 Arunachal Pradesh 1330 1239 3 3 91 0 0.23 Chandigarh 934 910 14 14 24 0 1.50 Meghalaya 779 738 5 5 41 0 0.64 Sikkim 592 568 1 1 24 0 0.17 Mizoram 384 361 0 0 23 0 0.00 Andaman and Nicobar Islands 359 334 1 1 25 0 0.28 Daman and Diu 0 0 0 0 0 0 NaN Lakshadweep 0 0 0 0 0 0 NaN .",
            "url": "https://anshuman6.github.io/anshuman-blog/2020/08/01/anshuman2.html",
            "relUrl": "/2020/08/01/anshuman2.html",
            "date": " • Aug 1, 2020"
        }
        
    
  

  
  

  

  
  

  
  

  
  

  
  

  
  

  
      ,"page7": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://anshuman6.github.io/anshuman-blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}